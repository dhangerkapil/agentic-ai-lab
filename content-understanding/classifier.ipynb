{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è Azure AI Content Understanding - Classifier and Analyzer Demo\n",
    "\n",
    "###  Background\n",
    "At Contoso Insurance our customers upload their claim forms to our website from our customer portal or fax them to a 800 number that drops them into the same location.\n",
    "We find that they often create a single file that contains not only the claim document but also all the supporting douments.\n",
    "Because this may include statements, bills and receipts from many different doctors, hospitals and labs, there is no way to extract from the varied document types without parsing the raw OCR data or creating custom input templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A Solution - Content Understanding  \n",
    "Allows for schema based extraction from pre-classified document types even when they are bundled into one file!\n",
    "\n",
    "This notebook demonstrates how to use the Azure [AI Content Understanding](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/overview) service to:\n",
    "1. Create a classifier to categorize bundled documents inside a single PDF file\n",
    "2. Create 3 custom analyzers to extract specific fields from specific document types.\n",
    "3. Combine classifier and the 3 analyzers to create an enhanced classier\n",
    "4. Classify, logically split, and analyze multiple documents bundled in a single PDF file.\n",
    "   \n",
    "If you‚Äôd like to learn more before getting started, see the official documentation:  \n",
    " - [Content Understanding classifier](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/classifier)  \n",
    " - [Understanding Analyzers in Azure AI Services](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/analyzer-templates?tabs=document)  \n",
    " - [Azure AI Content Understanding document solutions](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/overview)  \n",
    "\n",
    "## 0. Prerequisites  \n",
    "To get started, make sure you have the following resources and permissions:\n",
    "\n",
    "1. An Azure subscription. If you don't have an Azure subscription, create a free account.\n",
    "2. <mark>Create an AI Foundry Resource in the Azure Portal in one of the following supported regions: westus, swedencentral, or australiaeast.</mark>    \n",
    "This will be your Content Understanding Resource.  \n",
    "\n",
    "**FYI only -  no need to do this**  \n",
    "If you wanted to integrate your Content Understanding Resource with an AI Foundry project, you would  \n",
    "\n",
    "3. From the Azure Portal create an Azure AI Foundry hub-based project created in one of the following supported regions: westus, swedencentral, or australiaeast.    \n",
    "4. Create a project from the home page of AI Foundry Studio, or the Content Understanding landing page.  \n",
    "5. Add the Content Understanding Resource as a connected resource to the project.\n",
    "  \n",
    "  Keep track of the [AI Content Understanding release notes](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/whats-new) to know when Content Understanding becomes fully integrated with the new type of AI Foundry projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Azure Content Understanding Client\n",
    "\n",
    "The `AzureContentUnderstandingClient` class handles all API interactions with the Azure AI service.  \n",
    "There is not currently an official SDK for Content Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from content_understanding_client import AzureContentUnderstandingClient\n",
    "    print(\"‚úÖ Azure Content Understanding Client imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: Make sure 'AzureContentUnderstandingClient.py' is in the same directory as this notebook.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Environment Variables and set credentials\n",
    "\n",
    "Before executing this cell the first time  \n",
    "Modify your .env file to add the following entries at the bottom  \n",
    "  \n",
    "SERVICE_FOR_CU = The endpoint of the Azure AI Services endpoint in Foundry  \n",
    "SERVICE_API_FOR_CU = The api version for the service - 2025-05-01-preview will work  \n",
    "SAMPLE_CLAIMS_BUNDLE = The path to \"Data/sample claim submission.pdf\"   \n",
    "\n",
    "\n",
    "<img src=\"../images/cu-endpoint.png\" alt=\"Env Vars for Content Understanding\" width=\"70%\"/>\n",
    "\n",
    "**Save the .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always refresh all vars\n",
    "load_dotenv(override=True)\n",
    "# For authentication, we will use token-based auth. You can use key based auth, only one of them is required\n",
    "content_understanding_endpoint = os.getenv(\"SERVICE_FOR_CU\")\n",
    "# retrieve the API version to use\n",
    "content_understanding_api_version = os.getenv(\"SERVICE_API_FOR_CU\")\n",
    "# retrieve the path of the sample bundle file\n",
    "SAMPLE_CLAIMS_BUNDLE = os.getenv(\"SAMPLE_CLAIMS_BUNDLE\")\n",
    "print(SAMPLE_CLAIMS_BUNDLE)\n",
    "\n",
    "# Setup credentials\n",
    "credential = DefaultAzureCredential(\n",
    "    exclude_managed_identity_credential=True,\n",
    "    exclude_client_secret_credential=True,\n",
    "    exclude_environment_credential=True,\n",
    "    exclude_workload_identity_credential=True,\n",
    "    exclude_shared_token_cache_credential=True,\n",
    "    exclude_azure_powershell_credential=True,\n",
    "    exclude_azure_developer_cli_credential=True,\n",
    ")\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "file_location = Path(SAMPLE_CLAIMS_BUNDLE)\n",
    "\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"   Content Understanding Endpoint: {content_understanding_endpoint}\")\n",
    "print(f\"   Content Understanding API Version: {content_understanding_api_version}\")\n",
    "print(f\"   Document to Analyze: {file_location.name if file_location.exists() else '‚ùå File not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example of Defining a Classifier Schema\n",
    "\n",
    "The classifier schema defines:\n",
    "  - **Categories**: Document types to classify (e.g., Legal, Medical)\n",
    "  - **description (Optional)**: An optional field used to provide additional context or hints for categorizing or splitting documents.   \n",
    "    - This can be helpful when the category name alone isn‚Äôt descriptive enough. If the category name is already clear and self-explanatory, this field can be omitted.\n",
    "\n",
    "- **This classifier should indtify these document types**\n",
    "  -  Completed_Claim_Form  \n",
    "  -   HIPAA_Release  \n",
    "  -   Signed_Physician_Statement  \n",
    "  -   Pathology_Report  \n",
    "  -   Doctor_Office_Visit_Report  \n",
    "  -   Scanner_Report  \n",
    "  -   Other_Document_Type\n",
    "  -   Itemized_Bill_for_Lab_Services  \n",
    "  -   Itemized_Bill_for_Radiology_Services  \n",
    "  -   Itemized_Bill_from_Other_Service_Providers_Type  \n",
    "  -   UB04_Bil \n",
    "\n",
    "- **splitMode Options**: Defines how multi-page documents should be split before classification or analysis. There are 3 options:  \n",
    "  - `\"auto\"`: Automatically split based on content.  \n",
    "    - For example, if two categories are defined as ‚Äúinvoice‚Äù and ‚Äúapplication form‚Äù:\n",
    "       - A PDF with only one invoice will be classified as a single document.\n",
    "       - A PDF containing two invoices and one application form will be automatically split into three classified sections.\n",
    "  - `\"none\"`: No splitting.  \n",
    "    - The entire multi-page document is treated as a single unit for classification and analysis.\n",
    "  - `\"perPage\"`: Split by page.  \n",
    "    - Each page is treated as a separate document. This is useful when you‚Äôve built custom analyzers designed to operate on a per-page basis.\n",
    "\n",
    "  **Below is my schema definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This classifier schema will automatically splitting by document based on content! That means per document within the bundle\n",
    "# It defines field descriptions and classifier document categories and their descriptions\n",
    "classifier_schema = {\n",
    "\t\t\t\"categories\": {\n",
    "\t\t\t\t\t\"Completed_Claim_Form\": {\"description\": \"a Completed Claim Form\"},\n",
    "\t\t\t\t\t\"HIPAA_Release\": {\"description\": \"a HIPAA Release\"},\n",
    "\t\t\t\t\t\"Signed_Physician_Statement\": {\"description\": \"a Signed Physician Statement\"},\n",
    "                    \"Itemized_Bill_for_Lab_Services\": {\"description\": \"an Itemized Bill from a laboratory for Lab test and services. This type which includes Statments, Invoices, Account Summaries, any document that has dollar amounts on it sent from a lab\"},\n",
    "\t\t\t\t\t\"Itemized_Bill_for_Radiology_Services\": {\"description\": \"an Itemized Bill from a radiology department for imaging services. This type which includes Statments, Invoices, Account Summaries, any document that has dollar amounts on it sent from a radiology provider.\"},\n",
    "                    \"Itemized_Bill_from_Other_Service_Providers_Type\": {\"description\": \"an Itemized Bill from a other than a laboratory, a radiology provider or a hospitals provider types listed above This type which includes Statments, Invoices, Account Summaries, any document that has dollar amounts on it.\"},\n",
    "\t\t\t\t\t\"UB04_Bill\": {\"description\": \"A special type of itemized bill. It will have the notation on it UB04 or UB-04 or UB 04.\"},\n",
    "\t\t\t\t\t\"Pathology_Report\": {\"description\": \"a Pathology Report\"},\n",
    "                    \"Doctor_Office_Visit_Report\": {\"description\": \"a Doctor Office Visit Report contains a narrative of the visit, including symptoms, diagnosis, and treatment plan. It does not include any billing information.\"},\n",
    "                    \"Scanner_Report\": {\"description\": \"a Scanner Report that list issue with the scan of the documents\"},\n",
    "\t\t\t\t\t\"Other_Document_Type\": {\"description\": \"A document type other the other ones specified\"}\n",
    "\t\t\t\t\t},\n",
    "    \t\t\t\"splitMode\": \"auto\"  # IMPORTANT: Automatically detect document boundaries. Can change mode for your needs.\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\n",
    "# List out the doc types here\n",
    "print(\"üìÑ Classifier DocTypes:\")\n",
    "for category, details in classifier_schema[\"categories\"].items():\n",
    "    print(f\"   ‚Ä¢ {category}: {details['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Content Understanding Client\n",
    "\n",
    "Create the client that will communicate with Azure AI services.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "You must update the code below to match your Azure authentication method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure Content Understanding client\n",
    "try:\n",
    "    content_understanding_client = AzureContentUnderstandingClient(\n",
    "        endpoint=content_understanding_endpoint,\n",
    "        api_version=content_understanding_api_version,\n",
    "        token_provider=token_provider,\n",
    "    )\n",
    "    print(\"‚úÖ Content Understanding client initialized successfully!\")\n",
    "    print(\"   Ready to create classifiers and analyzers.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6A. Create a Custom Analyzer (analyzer_6A) for the itemizated bill doc types\n",
    "\n",
    "Now let's create a schema for a custom analyzer that can extract specific fields from documents.\n",
    "This analyzer will:\n",
    "- Extract the title from each of the documents in the bundle\n",
    "\n",
    "Note: we are defining the schema here, we will specify what document type to use this schema with down below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analyzer schema with custom fields\n",
    "analyzer_schema_analyzer_6A = {\n",
    "    \"description\": \"Analyzer_with_document_fields - extracts key document information from a bundle of documents in a single pdf submitted for claims\",\n",
    "    \"baseAnalyzerId\": \"prebuilt-documentAnalyzer\",  # Built on top of the general document analyzer\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,          # Extract layout information\n",
    "        \"enableBarcode\": False,        # Skip barcode detection\n",
    "        \"enableFormula\": False,        # Skip formula detection\n",
    "        \"estimateFieldSourceAndConfidence\": True, # Set to True if you want to estimate the field location (aka grounding) and confidence\n",
    "        \"disableContentFiltering\": False,\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "\t\t\t\"title_on_first_page_of_document\": {\n",
    "\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\"method\": \"generate\",\n",
    "\t\t\t\t\"description\": \"This is the title of the document. It will typically be the line of text with the largest sized font near the top of the page The value should be \\\"None\\\" if there is no title or it cannot be determined. \"\n",
    "\t\t\t}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate unique analyzer ID\n",
    "analyzer_id_analyzer_6A = \"Analyzer_with_document_fields_analyzer_6A_\" + str(uuid.uuid4())\n",
    "\n",
    "# Create the analyzer\n",
    "try:\n",
    "    print(f\"üî® Creating custom analyzer 6A: {analyzer_id_analyzer_6A}\")\n",
    "    print(\"\\nüìã Analyzer will extract:\")\n",
    "    for field_name, field_info in analyzer_schema_analyzer_6A[\"fieldSchema\"][\"fields\"].items():\n",
    "        print(f\"   ‚Ä¢ {field_name}: {field_info['description']}\")\n",
    "    \n",
    "    response = content_understanding_client.begin_create_analyzer(analyzer_id_analyzer_6A, analyzer_schema_analyzer_6A)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    # just printing the fields created\n",
    "    print(\"\\n‚úÖ Analyzer_with_document_fields created successfully!\")\n",
    "    print(f\"   Analyzer ID analyzer_6A: {analyzer_id_analyzer_6A}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating analyzer: {e}\")\n",
    "    analyzer_id_analyzer_6A = None  # Set to None if creation failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B. Create a Custom Analyzer (analyzer_6B) for the billed expenses\n",
    "\n",
    "Next let's create a schema for a custom analyzer that can extract specific fields from documents.\n",
    "This analyzer will:\n",
    "- Extract the all the billed expenses from each of the documents in the bundle as a table (array)  \n",
    "\n",
    "Note: we are defining the schema here, we will specify what document type to use this schema with down below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analyzer schema with custom fields\n",
    "analyzer_schema_analyzer_6B = {\n",
    "    \"description\": \"Analyzer_with_document_fields - extracts key document information from a bundle of documents in a single pdf submitted for claims\",\n",
    "    \"baseAnalyzerId\": \"prebuilt-documentAnalyzer\",  # Built on top of the general document analyzer\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,          # Extract layout information\n",
    "        \"enableBarcode\": False,        # Skip barcode detection\n",
    "        \"enableFormula\": False,        # Skip formula detection\n",
    "        \"estimateFieldSourceAndConfidence\": True, # Set to True if you want to estimate the field location (aka grounding) and confidence\n",
    "        \"disableContentFiltering\": False,\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "\t\t\t\"title_on_first_page_of_document\": {\n",
    "\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\"method\": \"generate\",\n",
    "\t\t\t\t\"description\": \"This is the title of the document. It will typically be the line of text with the largest sized font near the top of the page The value should be \\\"None\\\" if there is no title or it cannot be determined. \"\n",
    "\t\t\t},\n",
    "\t\t\t\"Expenses\": {\n",
    "\t\t\t\t\"type\": \"array\",\n",
    "\t\t\t\t\"items\": {\n",
    "\t\t\t\t\t\"type\": \"object\",\n",
    "\t\t\t\t\t\"properties\": {\n",
    "                        \"Expense_Amount\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"A table of the expense items amounts billed to patient or insurance company. These are charges for procedures, professional services, lab tests performed and other medical services. They will be numeric with 2 decimal places. Keep the 2 decimal places even it they are .00. They will typically be on the document pages in a tabular layout with the expensed dollar amounts all in the same column. You will typically find the other columns to extract ICD code CPT code etc for the other columns in this table usually on the same line as the amount. Only capture positive amounts that are actual charges (not totals, subtotals, adjustments, refunds, or negative values or amount that are zero). All dollar amounts for expenses must be captured. the document may contain multiple pages of expenses within a single document.\"\n",
    "                            },\n",
    "                            \"ICD_Code\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"The ICD code associated with the expense if there is one. If there is no ICD code, use \\\"\\\".  The ICD code is usually on the same line of the table as the amount\"\n",
    "                            },\n",
    "                            \"Date\": {\n",
    "                                \"type\": \"date\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"The date of the expense. The date is usually on the same line of the table as the amount format the date as mm/dd/yyyy.\"\n",
    "                            },\n",
    "                            \"Expense_Description\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"The description of the expense. This may be the procedure name. It is usually on the same line of the table as the amount.\"\t\n",
    "                            },\n",
    "                            \"Surgeon_Name_or_Provider\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"The surgeon or provider if this expense was a sugical procedure.\"\n",
    "                            },\n",
    "                            \"CPT_Code\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"The CPT code associated with the expense.  It is usually on the same line of the table as the amount.\"\t\n",
    "                            },\n",
    "                            \"Ref_Page\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"The page number with the document.\"\n",
    "                            },\n",
    "                            \"Drug_Name\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"If the expense charge was for a drug, put the drug name here. If not for a drug put N/A in this field.\"\n",
    "                            },\n",
    "                            \"Expense_Type\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"method\": \"generate\",\n",
    "                                \"description\": \"Categorize each expense into one of four categories based on the description, ICD10, CPT code, or other context. The 4 categories are:  1. Cancer_History_Expenses, 2. Diagnostic_Tests_and_Labs_Expenses,  3. Surgical_Events_Expenses,  4. Cancer_Treatment_Expenses. Put every expense into one of the four. If it was for a exam, a lab test or other diagostic test that diagosed cancer or remission, make it a #1. it was for a lab or dignostic test make it a #2 If it was for a surgical procedure make it a #3. Everything else is a #4. use the full name not just the number when filling in field.\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"method\": \"generate\"\n",
    "                    },\n",
    "\t\t\t\t\"method\": \"generate\",\n",
    "\t\t\t\t\"description\": \"Expenses are charges billed to either the patient or insurance company. They are single charges for a procedure, test or other medical service. They do not include payments, adjustments, refunds, total balances, subtotals. Other than these exceptions all other dollar amounts may be an expense and should be reviewed.\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "}\n",
    "\n",
    "# Generate unique analyzer ID\n",
    "analyzer_id_analyzer_6B = \"Analyzer_with_document_fields_analyzer_6B_\" + str(uuid.uuid4())\n",
    "\n",
    "# Create the analyzer\n",
    "try:\n",
    "    print(f\"üî® Creating custom analyzer 6B: {analyzer_id_analyzer_6B}\")\n",
    "    print(\"\\nüìã Analyzer will extract:\")\n",
    "    for field_name, field_info in analyzer_schema_analyzer_6B[\"fieldSchema\"][\"fields\"].items():\n",
    "        print(f\"   ‚Ä¢ {field_name}: {field_info['description']}\")\n",
    "\n",
    "    response = content_understanding_client.begin_create_analyzer(analyzer_id_analyzer_6B, analyzer_schema_analyzer_6B)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    # just printing the fields created\n",
    "    print(\"\\n‚úÖ Analyzer_with_document_fields created successfully!\")\n",
    "    print(f\"   Analyzer ID analyzer_6B: {analyzer_id_analyzer_6B}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating analyzer: {e}\")\n",
    "    analyzer_id_analyzer_6B = None  # Set to None if creation failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6C. Create a Custom Analyzer (analyzer_6C) for the patient information\n",
    "\n",
    "Next let's create a schema for a custom analyzer that can extract specific fields from documents.\n",
    "This analyzer will:\n",
    "- Extract the all the patient information  \n",
    "\n",
    "Note: we are defining the schema here, we will specify what document type to use this schema with down below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analyzer schema with custom fields\n",
    "analyzer_schema_analyzer_6C = {\n",
    "    \"description\": \"Analyzer_with_document_fields - extracts key document information from a bundle of documents in a single pdf submitted for claims\",\n",
    "    \"baseAnalyzerId\": \"prebuilt-documentAnalyzer\",  # Built on top of the general document analyzer\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,          # Extract layout information\n",
    "        \"enableBarcode\": False,        # Skip barcode detection\n",
    "        \"enableFormula\": False,        # Skip formula detection\n",
    "        \"estimateFieldSourceAndConfidence\": True, # Set to True if you want to estimate the field location (aka grounding) and confidence\n",
    "        \"disableContentFiltering\": False,\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "\t\t\t\"title_on_first_page_of_document\": {\n",
    "\t\t\t\t\"type\": \"string\",\n",
    "\t\t\t\t\"method\": \"generate\",\n",
    "\t\t\t\t\"description\": \"This is the title of the document. It will typically be the line of text with the largest sized font near the top of the page The value should be \\\"None\\\" if there is no title or it cannot be determined. \"\n",
    "\t\t\t},\n",
    "\t\t\t\"Patient_First_Name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The first name of the patient. This is usually on the first page of the document.\"\n",
    "            },\n",
    "            \"Patient_Last_Name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The last name of the patient. This is usually on the first page of the document.\"\n",
    "            },\n",
    "            \"DOB\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The DOB of the patient. This is usually on the first page of the document. Put into YYYY-MM-DD format.\"\n",
    "            },\n",
    "            \"Gender\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The gender of the patient. This is usually on the first page of the document.\"\n",
    "            },\n",
    "            \"Policy_Number\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The policy number of the patient. This is usually on the first page of the document. If the field is missing, use \\\"\\\".\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate unique analyzer ID\n",
    "analyzer_id_analyzer_6C = \"Analyzer_with_document_fields_analyzer_6C_\" + str(uuid.uuid4())\n",
    "\n",
    "# Create the analyzer\n",
    "try:\n",
    "    print(f\"üî® Creating custom analyzer: {analyzer_id_analyzer_6C}\")\n",
    "    print(\"\\nüìã Analyzer will extract:\")\n",
    "    for field_name, field_info in analyzer_schema_analyzer_6C[\"fieldSchema\"][\"fields\"].items():\n",
    "        print(f\"   ‚Ä¢ {field_name}: {field_info['description']}\")\n",
    "\n",
    "    response = content_understanding_client.begin_create_analyzer(analyzer_id_analyzer_6C, analyzer_schema_analyzer_6C)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    # just printing the fields created\n",
    "    print(\"\\n‚úÖ Analyzer_with_document_fields created successfully!\")\n",
    "    print(f\"   Analyzer ID: {analyzer_id_analyzer_6C}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating analyzer: {e}\")\n",
    "    analyzer_id_analyzer_6C = None  # Set to None if creation failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create an Enhanced Classifier Schema with 3 Custom Analyzers\n",
    "\n",
    "Now we'll create a new enhanced classifier Schema that classifies the documents and also specifies which of our 3 custom analyzers to use to extract fields with on each of the document types defined.  \n",
    "This combines classification with field extraction in one operation.  \n",
    "\n",
    "We are using the 3 analyzers from previos cells analyzer_6A, analyzer_6B and analyzer_6C\n",
    "\n",
    "\n",
    "**This is now the schema my Classifier will use**  \n",
    "It will be used instead of the one created in the earlier step #4 because it defines the analyzer to use for each document type.  \n",
    "\n",
    "Note: For each document type, one of the 3 analyzers: analyzer_6A or analyzer_6B or analyzer_6C is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split based on content! That means per document within the bundle\n",
    "# Define field descriptions and classifier document categories and their descriptions.\n",
    "# Notice that the analyzer to use is specified for each document type.\n",
    "enhanced_classifier_with_document_metadata_and_fields_schema = {\n",
    "\t\t\t\"categories\": {\n",
    "                \t\"Completed_Claim_Form\": {\"description\": \"a Completed Claim Form\", \"analyzerId\": analyzer_id_analyzer_6C},\n",
    "\t\t\t\t\t\"HIPAA_Release\": {\"description\": \"a HIPAA Release\", \"analyzerId\": analyzer_id_analyzer_6A},\n",
    "\t\t\t\t\t\"Signed_Physician_Statement\": {\"description\": \"a Signed Physician Statement\", \"analyzerId\": analyzer_id_analyzer_6A},\n",
    "                    \"Pathology_Report\": {\"description\": \"a Pathology Report\", \"analyzerId\": analyzer_id_analyzer_6A},\n",
    "                    \"Doctor_Office_Visit_Report\": {\"description\": \"a Doctor Office Visit Report contains a narrative of the visit, including symptoms, diagnosis, and treatment plan. It does not include any billing information.\", \"analyzerId\": analyzer_id_analyzer_6A},\n",
    "                    \"Scanner_Report\": {\"description\": \"a Scanner Report that list issue with the scan of the documents\", \"analyzerId\": analyzer_id_analyzer_6A},\n",
    "\t\t\t\t\t\"Other_Document_Type\": {\"description\": \"A document type other the other ones specified\", \"analyzerId\": analyzer_id_analyzer_6A},\n",
    "\t\t\t\t\t\"Itemized_Bill_for_Lab_Services\": {\"description\": \"an Itemized Bill from a laboratory for Lab test and services. This type which includes Statments, Invoices, Account Summaries, any document that has dollar amounts on it sent from a lab\", \"analyzerId\": analyzer_id_analyzer_6B},\n",
    "\t\t\t\t\t\"Itemized_Bill_for_Radiology_Services\": {\"description\": \"an Itemized Bill from a radiology department for imaging services. This type which includes Statments, Invoices, Account Summaries, any document that has dollar amounts on it sent from a radiology provider.\", \"analyzerId\": analyzer_id_analyzer_6B},\n",
    "                    \"Itemized_Bill_from_Other_Service_Providers_Type\": {\"description\": \"an Itemized Bill from a other than a laboratory, a radiology provider or a hospitals provider types listed above This type which includes Statments, Invoices, Account Summaries, any document that has dollar amounts on it.\", \"analyzerId\": analyzer_id_analyzer_6B},\n",
    "\t\t\t\t\t\"UB04_Bill\": {\"description\": \"A special type of itemized bill. It will have the notation on it UB04 or UB-04 or UB 04.\", \"analyzerId\": analyzer_id_analyzer_6B},\n",
    "\t\t\t\t\t},\n",
    "    \t\t\t\"splitMode\": \"auto\"  # IMPORTANT: Automatically detect document boundaries. Can change mode for your needs.\n",
    "\t\t\t}\n",
    "# Just printing out my doc types here\n",
    "print(\"üìÑ Classifier DocTypes:\")\n",
    "for category, details in enhanced_classifier_with_document_metadata_and_fields_schema[\"categories\"].items():\n",
    "    print(f\"   ‚Ä¢ {category}: {details['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Then, create the Classifier  \n",
    "\n",
    "It takes the Classifier Schema as an input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the analyzer ids created in prior cells\n",
    "print(f\"Using analyzer from prior cell analyzer_6A:: {analyzer_id_analyzer_6A}\")\n",
    "print(f\"Using analyzer from prior cell analyzer_6B:: {analyzer_id_analyzer_6B}\")\n",
    "print(f\"Using analyzer from prior cell analyzer_6C:: {analyzer_id_analyzer_6C}\")\n",
    "\n",
    "# This is the classifier we are creating now\n",
    "# Generate unique enhanced classifier ID\n",
    "enhanced_classifier_id = \"classifier_based_on_doc_type_9\" + str(uuid.uuid4())\n",
    "print(f\"üî® Creating classifier: {enhanced_classifier_id}\")\n",
    "\n",
    "# Create the enhanced classifier\n",
    "if analyzer_id_analyzer_6A and analyzer_id_analyzer_6B and analyzer_id_analyzer_6C:  # Only create if all of the previous analyzers were successfully created\n",
    "\ttry:\n",
    "\t\tresponse = content_understanding_client.begin_create_classifier(enhanced_classifier_id, enhanced_classifier_with_document_metadata_and_fields_schema )\n",
    "\t\tresult = content_understanding_client.poll_result(response)\n",
    "\t\t\t\n",
    "\t\tprint(\"\\n‚úÖ Enhanced classifier created successfully!\")\n",
    "\t\tprint(\"\\nüìã Configuration:\")\n",
    "\t\tprint(\"   ‚Ä¢ Medical documents in claim bundle ‚Üí 3 Custom analyzers with field extraction ‚Üí 1 enhanced Classifier\")\n",
    "\n",
    "\t\tprint(f\"\\n   ‚Ä¢ These document types below can use the classifier {enhanced_classifier_id} and the custom analyzer - analyzer_id: {analyzer_id_analyzer_6C} and this schema:  enhanced_classifier_with_document_metadata_and_fields_schema\")\n",
    "\t\tprint(\"\\n\t- Completed_Claim_Form\")\n",
    "\n",
    "\n",
    "\t\tprint(f\"\\n   ‚Ä¢ These document types below can use the classifier {enhanced_classifier_id} and the custom analyzer - analyzer_id: {analyzer_id_analyzer_6A} and this schema:  enhanced_classifier_with_document_metadata_and_fields_schema\")\n",
    "\t\tprint(\"\\n\t  - HIPAA_Release\")\n",
    "\t\tprint(\"\t  - Signed_Physician_Statement\")\n",
    "\t\tprint(\"\t  - Pathology_Report\")\n",
    "\t\tprint(\"\t  - Doctor_Office_Visit_Report\")\n",
    "\t\tprint(\"\t  - Scanner_Report\")\n",
    "\t\tprint(\"\t  - Other_Document_Type\")\n",
    "\n",
    "\t\tprint(f\"\\n   ‚Ä¢ These document types below can use the classifier {enhanced_classifier_id} and the custom analyzer - analyzer_id: {analyzer_id_analyzer_6B} and this schema: enhanced_classifier_with_document_metadata_and_fields_schema\")\n",
    "\t\tprint(f\"\\n\t- Itemized_Bill_for_Lab_Services\")\n",
    "\t\tprint(f\"\t- Itemized_Bill_for_Radiology_Services\")\n",
    "\t\tprint(f\"\t- Itemized_Bill_from_Other_Service_Providers_Type\")\n",
    "\t\tprint(f\"\t- UB04_Bill\")\n",
    "\t\t\t\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"\\n‚ùå Error creating enhanced classifier: {e}\")\n",
    "else:\n",
    "\tprint(\"‚ö†Ô∏è  Skipping enhanced classifier creation - analyzer was not created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Process Document with Enhanced Classifier \n",
    "\n",
    "**This step is:**\n",
    "1.  Reading the PDF documentbundle file  \n",
    "2.  Classifying the documents within it\n",
    "3.  Extracting the fields from the document based on document type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the classifier that breaks the bundle into documents\n",
    "#\n",
    "# Checking that all the analyzers were created\n",
    "if analyzer_id_analyzer_6A and analyzer_id_analyzer_6B and analyzer_id_analyzer_6C:\n",
    "    print(f\"üî® Using analyzer: {analyzer_id_analyzer_6A} and {analyzer_id_analyzer_6B}  and {analyzer_id_analyzer_6C}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping analyzer usage - analyzer was not created successfully in previous cell\")\n",
    "\n",
    "\n",
    "if enhanced_classifier_id and analyzer_id_analyzer_6A and analyzer_id_analyzer_6B and analyzer_id_analyzer_6C:\n",
    "    print(f\"üî® Using classifier: {enhanced_classifier_id}\")\n",
    "    try:\n",
    "        # Check if document exists\n",
    "        if not file_location.exists():\n",
    "            raise FileNotFoundError(f\"Document not found at {file_location}\")\n",
    "    \n",
    "        # Process with enhanced classifier\n",
    "        print(\"üìÑ Processing document with enhanced classifier\")\n",
    "        print(f\"   Document: {file_location.name}\")\n",
    "        print(\"\\n‚è≥ Processing with classification + field extraction...\")\n",
    "\n",
    "        response = content_understanding_client.begin_classify(classifier_id=enhanced_classifier_id, file_location=str(file_location))\n",
    "        enhanced_result = content_understanding_client.poll_result(response, timeout_seconds=920,polling_interval_seconds=25)\n",
    "        \n",
    "        print(\"\\n‚úÖ Enhanced processing completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing document: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping enhanced classification - enhanced classifier was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create a function to do the parsing and execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_and_display_results(enhanced_result):\n",
    "   \n",
    "    data = enhanced_result\n",
    "    # Extract the main result data\n",
    "    result_data = data.get(\"result\", {})\n",
    "    contents = result_data.get(\"contents\", [])\n",
    "    \n",
    "    print(\"\\nüìä DOCUMENT ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total sections(documents) found: {len(contents)}\")\n",
    "    \n",
    "    # Process each document section\n",
    "    for i, content in enumerate(contents, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"DOCUMENT #{i}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Basic document information\n",
    "        category = content.get('category', 'Unknown')\n",
    "        start_page = content.get('startPageNumber', '?')\n",
    "        end_page = content.get('endPageNumber', '?')\n",
    "        \n",
    "        # Calculate number of pages\n",
    "        if start_page != '?' and end_page != '?':\n",
    "            num_pages = end_page - start_page + 1\n",
    "        else:\n",
    "            num_pages = '?'\n",
    "        \n",
    "        print(f\"üìÅ Type of Document: {category}\")\n",
    "        print(f\"üìÑ Document Starting Page in Bundle: {start_page}\")\n",
    "        print(f\"üìÑ Document Ending Page in Bundle: {end_page}\")\n",
    "        print(f\"üìÑ Number of Pages in Document: {num_pages}\")\n",
    "        \n",
    "        # Extract and display fields\n",
    "        fields = content.get('fields', {})\n",
    "        field_count = len(fields)\n",
    "        print(f\"Fields extracted from this Document: {field_count}\")\n",
    "        \n",
    "        if fields:\n",
    "            # Handle title field\n",
    "            if 'title_on_first_page_of_document' in fields:\n",
    "                title_field = fields['title_on_first_page_of_document']\n",
    "                title_value = title_field.get('valueString', 'N/A')\n",
    "                print(f\"üìÑ Document Title: {title_value}\")\n",
    "            \n",
    "            # Handle patient information fields\n",
    "            patient_fields = ['Patient_First_Name', 'Patient_Last_Name', 'DOB', 'Gender', 'Policy_Number']\n",
    "            for field_name in patient_fields:\n",
    "                if field_name in fields:\n",
    "                    field_data = fields[field_name]\n",
    "                    field_value = field_data.get('valueString', 'N/A')\n",
    "                    print(f\"üìÑ {field_name}: {field_value}\")\n",
    "            \n",
    "            # Handle Expenses array\n",
    "            if 'Expenses' in fields:\n",
    "                expenses_field = fields['Expenses']\n",
    "                expenses_array = expenses_field.get('valueArray', [])\n",
    "                print(f\"üìÑ Expenses: Found {len(expenses_array)} expense entries\")\n",
    "                \n",
    "                for idx, expense in enumerate(expenses_array, 1):\n",
    "                    print(f\"    üí∞ Expense #{idx}:\")\n",
    "                    expense_obj = expense.get('valueObject', {})\n",
    "                    \n",
    "                    # Define expense fields to extract in order of importance\n",
    "                    expense_fields = [\n",
    "                        'Expense_Amount', 'Expense_Description', 'Date', 'CPT_Code',\n",
    "                        'ICD_Code', 'Expense_Type', 'Surgeon_Name_or_Provider', \n",
    "                        'Ref_Page', 'Drug_Name'\n",
    "                    ]\n",
    "                    \n",
    "                    for exp_field in expense_fields:\n",
    "                        if exp_field in expense_obj:\n",
    "                            field_data = expense_obj[exp_field]\n",
    "                            field_type = field_data.get('type', 'unknown')\n",
    "                            \n",
    "                            # Get value based on type\n",
    "                            if field_type == 'number':\n",
    "                                field_value = field_data.get('valueNumber', 'N/A')\n",
    "                                if exp_field == 'Expense_Amount':\n",
    "                                    field_value = f\"${field_value:.2f}\"\n",
    "                                if exp_field == 'Ref_Page':\n",
    "                                    field_value = f\"{field_value:.0f}\"\n",
    "                                    field_value = int(field_value)  # Convert to int for page number\n",
    "                                    field_value = field_value+start_page-1\n",
    "                            elif field_type == 'date':\n",
    "                                field_value = field_data.get('valueDate', 'N/A')\n",
    "                            else:\n",
    "                                field_value = field_data.get('valueString', 'N/A')\n",
    "                            \n",
    "                            print(f\"      üìÑ {exp_field}: {field_value}\")\n",
    "            \n",
    "            # Calculate word-level confidence statistics for this document\n",
    "            pages = content.get('pages', [])\n",
    "            if pages:\n",
    "                total_words = 0\n",
    "                total_confidence = 0\n",
    "                min_confidence = 1.0\n",
    "                max_confidence = 0.0\n",
    "                \n",
    "                for page in pages:\n",
    "                    words = page.get('words', [])\n",
    "                    for word in words:\n",
    "                        confidence = word.get('confidence', 0)\n",
    "                        if confidence > 0:  # Only count words with confidence scores\n",
    "                            total_words += 1\n",
    "                            total_confidence += confidence\n",
    "                            min_confidence = min(min_confidence, confidence)\n",
    "                            max_confidence = max(max_confidence, confidence)\n",
    "                \n",
    "                if total_words > 0:\n",
    "                    avg_confidence = total_confidence / total_words\n",
    "                    print(f\"\\nüìä Confidence Information:\")\n",
    "                    print(f\"   üìÑ Word-level confidence - Avg: {avg_confidence:.3f}, Min: {min_confidence:.3f}, Max: {max_confidence:.3f}\")\n",
    "                    print(f\"   üìÑ Total words with confidence scores: {total_words}\")\n",
    "                else:\n",
    "                    print(f\"\\nüìä Confidence Information: No word-level confidence scores available\")\n",
    "        \n",
    "        if field_count == 0:\n",
    "            print(\"   üìÑ No fields were extracted from this document\")\n",
    "\n",
    "# Call the function to parse and display the results\n",
    "parse_json_and_display_results(enhanced_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create a function to give a Summary and execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_summary(enhanced_result):\n",
    "    \"\"\"\n",
    "    Display a concise summary of the document analysis results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = enhanced_result\n",
    "        print(\"üìä DOCUMENT BUNDLE SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        result_data = data.get(\"result\", {})\n",
    "        contents = result_data.get(\"contents\", [])\n",
    "        if contents:  \n",
    "            last_end_page = contents[-1].get('endPageNumber', '?')  \n",
    "        else:  \n",
    "            last_end_page = '?'\n",
    "        \n",
    "        print(f\"Total documents found: {len(contents)}\")\n",
    "        print(f\"Total pages in bundle: {last_end_page}\")\n",
    "        \n",
    "        # Summary table\n",
    "        print(\"\\nüìã Document Summary:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'#':<3} {'Document Type':<50} {'Pages':<8} {'Fields':<8}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        total_expenses = 0\n",
    "        for i, content in enumerate(contents, 1):\n",
    "            category = content.get('category', 'Unknown')\n",
    "            start_page = content.get('startPageNumber', '?')\n",
    "            end_page = content.get('endPageNumber', '?')\n",
    "            \n",
    "            if start_page != '?' and end_page != '?':\n",
    "                page_range = f\"{start_page}-{end_page}\"\n",
    "                num_pages = end_page - start_page + 1\n",
    "            else:\n",
    "                page_range = '?'\n",
    "                num_pages = '?'\n",
    "            \n",
    "            fields = content.get('fields', {})\n",
    "            field_count = len(fields)\n",
    "            \n",
    "            # Count expenses\n",
    "            if 'Expenses' in fields:\n",
    "                expenses = fields['Expenses'].get('valueArray', [])\n",
    "                expense_count = len(expenses)\n",
    "                total_expenses += expense_count\n",
    "                field_info = f\"{field_count} (+{expense_count} expenses)\"\n",
    "            else:\n",
    "                field_info = str(field_count)\n",
    "            \n",
    "            print(f\"{i:<3} {category:<50} {page_range:<8} {field_info:<8}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\nüí∞ Total expenses found across all documents: {total_expenses}\")\n",
    "        \n",
    "        # Show which documents have patient info vs expenses\n",
    "        print(f\"\\nüìù Field Distribution:\")\n",
    "        print(f\"   ‚Ä¢ Insurance Claim Form: Patient information fields\")\n",
    "        print(f\"   ‚Ä¢ Billing Statements: Expense details + document titles\")\n",
    "        print(f\"   ‚Ä¢ Other Documents: Document titles only\")\n",
    "        \n",
    "        return contents\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"‚ùå Error: enhanced_result variable not found. Run the document processing cell first.\")\n",
    "        return None\n",
    "\n",
    "# Display the summary\n",
    "summary_data = parse_json_summary(enhanced_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Lab Summary \n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. ‚úÖ Created a basic classifier to categorize documents\n",
    "2. ‚úÖ Created a 3 custom analyzers to extract specific fields from specific types of documents\n",
    "3. ‚úÖ Combined them into an enhanced classifier for intelligent document processing\n",
    "4. ‚úÖ Processed a sample file using the enhanced classier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Content Understanding can do so much more than just documents. It can process audio, video and images too!\n",
    "\n",
    "[Learn more here: What is Azure AI Content Understanding (preview)?](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/overview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
