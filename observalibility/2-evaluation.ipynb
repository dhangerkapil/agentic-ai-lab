{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f23655a",
   "metadata": {},
   "source": [
    "# üçè Health Assistant Evaluation Demo üçé\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Foundry's evaluation capabilities to assess the quality and safety of AI-generated health and fitness responses.\n",
    "\n",
    "## üîê Authentication Setup\n",
    "\n",
    "Before running the next cell, make sure you're authenticated with Azure CLI. Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "az login --use-device-code\n",
    "```\n",
    "\n",
    "This will provide you with a device code and URL to authenticate in your browser, which is useful for:\n",
    "- Remote development environments\n",
    "- Systems without a default browser\n",
    "- Corporate environments with strict security policies\n",
    "\n",
    "After successful authentication, you can proceed with the notebook cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02495886",
   "metadata": {},
   "source": [
    "## üìä Available Evaluators in Azure AI Foundry\n",
    "\n",
    "Azure AI Foundry provides a comprehensive set of built-in evaluators for different aspects of AI model quality:\n",
    "\n",
    "### **AI Quality (AI Assisted)**\n",
    "- **Groundedness** - Measures how well responses are grounded in provided context\n",
    "- **Relevance** - Evaluates how relevant responses are to the input query  \n",
    "- **Coherence** - Assesses logical flow and consistency in responses\n",
    "- **Fluency** - Measures language quality and readability\n",
    "- **GPT Similarity** - Compares responses to reference answers\n",
    "\n",
    "### **AI Quality (NLP Metrics)**\n",
    "- **F1 Score** - Measures precision and recall balance\n",
    "- **ROUGE Score** - Evaluates text summarization quality\n",
    "- **BLEU Score** - Measures translation and generation quality\n",
    "- **GLEU Score** - Google's BLEU variant for better correlation\n",
    "- **METEOR Score** - Considers synonyms and stemming\n",
    "\n",
    "### **Risk and Safety**\n",
    "- **Violence** - Detects violent content\n",
    "- **Sexual** - Identifies sexual content\n",
    "- **Self-harm** - Detects self-harm related content\n",
    "- **Hate/Unfairness** - Identifies hateful or unfair content\n",
    "- **Protected Material** - Detects copyrighted content\n",
    "- **Indirect Attack** - Identifies indirect prompt injection attempts\n",
    "\n",
    "üìö **For complete details on all available evaluators, their parameters, and usage examples, visit:**  \n",
    "**[Azure AI Foundry Evaluators Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Azure AI Foundry Evaluations üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This notebook demonstrates how to evaluate AI models using Azure AI Foundry with both **local** and **cloud** evaluations.\n",
    "\n",
    "## What This Notebook Does:\n",
    "1. **Setup & Data Creation** - Creates synthetic health & fitness Q&A data\n",
    "2. **Local Evaluation** - Runs F1Score and Relevance evaluators locally  \n",
    "3. **Cloud Evaluation** - Uploads results to Azure AI Foundry project\n",
    "\n",
    "## Key Features:\n",
    "‚úÖ **Local Evaluations** - F1Score and AI-assisted Relevance evaluators\n",
    "‚úÖ **Cloud Integration** - Upload results to Azure AI Foundry\n",
    "‚úÖ **Browser Authentication** - Uses InteractiveBrowserCredential  \n",
    "‚úÖ **Error Handling** - Robust fallbacks and clear status reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup and Data Creation\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root directory\n",
    "root_env_path = os.environ.get(\"ROOT_ENV_PATH\", '../../../.env')\n",
    "load_dotenv(root_env_path)\n",
    "print(f\"‚úÖ Environment variables loaded from: {root_env_path}\")\n",
    "\n",
    "# Check required environment variables for Azure AI Foundry\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(\"üîç Environment Variables Status:\")\n",
    "print(\n",
    "    f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {'‚úÖ Set' if AI_FOUNDRY_PROJECT_ENDPOINT else '‚ùå Missing'}\"\n",
    ")\n",
    "print(f\"   TENANT_ID: {'‚úÖ Set' if TENANT_ID else '‚ùå Missing'}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"\\n‚ö†Ô∏è Required environment variables missing!\")\n",
    "    print(\"Please add these to your .env file:\")\n",
    "    print(\"AI_FOUNDRY_PROJECT_ENDPOINT=<your-azure-ai-project-endpoint>\")\n",
    "    print(\"TENANT_ID=<your-azure-tenant-id>\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All environment variables configured correctly!\")\n",
    "    print(f\"üîß Loaded values:\")\n",
    "    print(f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "    print(f\"   TENANT_ID: {TENANT_ID}\")\n",
    "\n",
    "# Create synthetic health & fitness evaluation data\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write data to JSONL file\n",
    "eval_data_filename = os.environ.get(\"EVAL_DATA_FILENAME\", \"health_fitness_eval_data.jsonl\")\n",
    "eval_data_path = Path(f\"./{eval_data_filename}\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Evaluation data created: {eval_data_path.resolve()}\")\n",
    "print(f\"üìä Total samples: {len(synthetic_eval_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "## üîç Local Evaluation\n",
    "\n",
    "Run evaluations locally using F1Score (basic text similarity) and Relevance (AI-assisted) evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Evaluation with Azure AI Foundry\n",
    "from azure.ai.evaluation import evaluate, F1ScoreEvaluator, RelevanceEvaluator\n",
    "import logging\n",
    "\n",
    "# Reduce logging noise\n",
    "logging.getLogger('promptflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('azure.ai.evaluation').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"üîç Running Local Evaluation...\")\n",
    "\n",
    "# Configure evaluators\n",
    "evaluators = {\n",
    "    \"f1_score\": F1ScoreEvaluator()\n",
    "}\n",
    "\n",
    "evaluator_config = {\n",
    "    \"f1_score\": {\n",
    "        \"column_mapping\": {\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add AI-assisted evaluator if Azure OpenAI is configured\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4\")),\n",
    "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", os.environ.get(\"API_VERSION\", \"2024-02-15-preview\")),\n",
    "}\n",
    "\n",
    "if model_config[\"azure_endpoint\"] and model_config[\"api_key\"]:\n",
    "    print(\"ü§ñ Adding AI-assisted Relevance evaluator...\")\n",
    "    evaluators[\"relevance\"] = RelevanceEvaluator(model_config=model_config)\n",
    "    evaluator_config[\"relevance\"] = {\n",
    "        \"column_mapping\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Azure OpenAI not configured - using F1Score only\")\n",
    "\n",
    "# Run local evaluation\n",
    "try:\n",
    "    local_result = evaluate(\n",
    "        data=str(eval_data_path),\n",
    "        evaluators=evaluators,\n",
    "        evaluator_config=evaluator_config\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Local evaluation completed!\")\n",
    "    \n",
    "    # Display results\n",
    "    metrics = local_result['metrics']\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"üìä {metric_name}: {value:.4f}\")\n",
    "        \n",
    "        # Save results locally\n",
    "        local_results_filename = os.environ.get(\"LOCAL_RESULTS_FILENAME\", \"local_evaluation_results.json\")\n",
    "        with open(local_results_filename, \"w\") as f:\n",
    "            json.dump(local_result, f, indent=2)\n",
    "\n",
    "        print(f\"üíæ Results saved to: {local_results_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local evaluation failed: {e}\")\n",
    "    local_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Cloud Evaluation\n",
    "\n",
    "Upload evaluation results to Azure AI Foundry project for tracking and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Evaluation - Following Official Microsoft Documentation\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"‚òÅÔ∏è Setting up Cloud Evaluation with Azure AI Foundry...\")\n",
    "\n",
    "# Configuration from environment variables\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(f\"üè¢ Foundry Project Endpoint: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "print(f\"üîë Tenant ID: {TENANT_ID}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"‚ö†Ô∏è Missing AI_FOUNDRY_PROJECT_ENDPOINT in .env file\")\n",
    "    cloud_result = None\n",
    "else:\n",
    "    try:\n",
    "        # Step 1: Create project client using DefaultAzureCredential (as per Microsoft docs)\n",
    "        print(\"üîê Setting up authentication...\")\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "\n",
    "        # Step 2: Upload evaluation data to Azure AI Foundry (required for cloud evaluation)\n",
    "        print(\"üì§ Uploading evaluation data to Azure AI Foundry...\")\n",
    "        dataset_name = os.environ.get(\"DATASET_NAME\", f\"health-fitness-dataset-{int(time.time())}\")\n",
    "        dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")\n",
    "        try:\n",
    "            data_upload = project_client.datasets.upload_file(\n",
    "                name=dataset_name,\n",
    "                version=dataset_version,\n",
    "                file_path=str(eval_data_path),\n",
    "            )\n",
    "            data_id = data_upload.id\n",
    "            print(f\"‚úÖ Data uploaded successfully! Dataset ID: {data_id}\")\n",
    "        except Exception as upload_error:\n",
    "            print(f\"‚ùå Data upload failed: {upload_error}\")\n",
    "            raise upload_error\n",
    "\n",
    "        # Step 3: Configure evaluators using Azure AI Foundry built-in evaluators\n",
    "        print(\"‚öôÔ∏è Configuring evaluators for cloud evaluation...\")\n",
    "\n",
    "        evaluators = {\n",
    "            \"bleu_score\": EvaluatorConfiguration(\n",
    "                id=EvaluatorIds.BLEU_SCORE.value,\n",
    "                data_mapping={\n",
    "                    \"response\": \"${data.response}\",\n",
    "                    \"ground_truth\": \"${data.ground_truth}\",\n",
    "                },\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Step 4: Create and submit evaluation\n",
    "        print(\"üöÄ Creating and submitting cloud evaluation...\")\n",
    "        evaluation_name = os.environ.get(\"EVALUATION_NAME\", f\"health-fitness-eval-{int(time.time())}\")\n",
    "        evaluation = Evaluation(\n",
    "            display_name=evaluation_name,\n",
    "            description=\"Health and fitness AI response evaluation\",\n",
    "            data=InputDataset(id=data_id),\n",
    "            evaluators=evaluators,\n",
    "        )\n",
    "\n",
    "        # Submit the evaluation\n",
    "        evaluation_response = project_client.evaluations.create(evaluation)\n",
    "\n",
    "        print(\"üéâ CLOUD EVALUATION SUBMITTED!\")\n",
    "        print(f\"   üìã Name: {evaluation_response.name}\")\n",
    "        print(f\"   üìã Status: {evaluation_response.status}\")\n",
    "        print(f\"   üìã Response Type: {type(evaluation_response)}\")\n",
    "\n",
    "        # Get evaluation ID - handle different possible attribute names\n",
    "        evaluation_id = None\n",
    "        if hasattr(evaluation_response, 'id'):\n",
    "            evaluation_id = evaluation_response.id\n",
    "        elif hasattr(evaluation_response, 'name'):\n",
    "            evaluation_id = evaluation_response.name  # Use name as ID if no separate ID exists\n",
    "\n",
    "        if evaluation_id:\n",
    "            print(f\"   üìã ID: {evaluation_id}\")\n",
    "\n",
    "        print(f\"\\nüîó View detailed results at: https://ai.azure.com/\")\n",
    "        print(\"   Navigate to your project ‚Üí Evaluation ‚Üí View evaluation runs\")\n",
    "\n",
    "        # Save results\n",
    "        cloud_result = {\n",
    "            \"evaluation_name\": evaluation_response.name,\n",
    "            \"status\": evaluation_response.status,\n",
    "            \"project_endpoint\": AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            \"dataset_id\": data_id,\n",
    "            \"timestamp\": int(time.time()),\n",
    "        }\n",
    "\n",
    "        # Add evaluation ID if available\n",
    "        if evaluation_id:\n",
    "            cloud_result[\"evaluation_id\"] = evaluation_id\n",
    "\n",
    "        with open(os.environ.get(\"CLOUD_RESULTS_FILENAME\", \"cloud_evaluation_results.json\"), \"w\") as f:\n",
    "            json.dump(cloud_result, f, indent=2, default=str)\n",
    "        print(f\"üíæ Results saved to: {os.environ.get('CLOUD_RESULTS_FILENAME', 'cloud_evaluation_results.json')}\")\n",
    "\n",
    "        print(\"\\n‚úÖ SUCCESS: Cloud evaluation submitted to Azure AI Foundry!\")\n",
    "        print(\"   The evaluation will run in the cloud and results will be available in the Azure AI Foundry portal.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cloud evaluation failed: {e}\")\n",
    "        print(f\"üìã Error type: {type(e).__name__}\")\n",
    "\n",
    "        # Enhanced error handling\n",
    "        error_str = str(e).lower()\n",
    "        if \"401\" in error_str or \"unauthorized\" in error_str:\n",
    "            print(\"\\nüîê AUTHENTICATION ISSUE:\")\n",
    "            print(\"   - Make sure you're logged in with: az login\")\n",
    "            print(\"   - Ensure you have access to the Azure AI Foundry project\")\n",
    "        elif \"403\" in error_str or \"forbidden\" in error_str:\n",
    "            print(\"\\nüö´ PERMISSION ISSUE:\")\n",
    "            print(\"   - Verify you have 'AI Developer' or 'Contributor' role\")\n",
    "            print(\"   - Check Azure AI Foundry project permissions\")\n",
    "        elif \"404\" in error_str or \"not found\" in error_str:\n",
    "            print(\"\\nüîç RESOURCE NOT FOUND:\")\n",
    "            print(\"   - Verify AI_FOUNDRY_PROJECT_ENDPOINT is correct\")\n",
    "            print(\"   - Check if project exists in Azure AI Foundry\")\n",
    "        elif \"storage\" in error_str or \"blob\" in error_str:\n",
    "            print(\"\\nüíæ STORAGE ISSUE:\")\n",
    "            print(\"   - Ensure your Azure AI Foundry project has a connected storage account\")\n",
    "            print(\"   - Check storage account permissions for the project\")\n",
    "        else:\n",
    "            print(f\"\\nüí° TROUBLESHOOTING:\")\n",
    "            print(f\"   - Full error: {str(e)[:300]}...\")\n",
    "            print(\"   - Try running local evaluation first\")\n",
    "            print(\"   - Check Azure AI Foundry project configuration\")\n",
    "\n",
    "        cloud_result = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
