{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f0b7d7",
   "metadata": {},
   "source": [
    "# üçè Observability & Tracing Demo with `azure-ai-projects` and `azure-ai-inference` üçé\n",
    "\n",
    "> üìö **For developers and learners**: Refer to the official Azure AI Foundry observability documentation: [https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)\n",
    "\n",
    "Welcome to this **Health & Fitness**-themed notebook, where we'll explore how to set up **observability** and **tracing** for:\n",
    "\n",
    "1. **Basic LLM calls** using an `AIProjectClient`.\n",
    "2. **Multi-step** interactions using an **Agent** (such as a Health Resource Agent).\n",
    "3. **Tracing** your local usage in **console** (stdout) or via an **OTLP endpoint** (like **Prompty** or **Aspire**).\n",
    "4. Sending those **traces** to **Azure Monitor** (Application Insights) so you can view them in **Azure AI Foundry**.\n",
    "\n",
    "> **Disclaimer**: This is a fun demonstration of AI and observability! Any references to workouts, diets, or health routines in the code or prompts are purely for **educational** purposes. Always consult a professional for health advice.\n",
    "\n",
    "## Contents\n",
    "1. **Initialization**: Setting up environment, creating clients.\n",
    "2. **Basic LLM Call**: Quick demonstration of retrieving model completions.\n",
    "3. **Connections**: Listing project connections.\n",
    "4. **Observability & Tracing**\n",
    "   - **Azure Monitor** tracing: hooking up to Application Insights\n",
    "   - **Verifying** your traces in Azure AI Foundry\n",
    "5. **Agent-based Example**:\n",
    "   - Creating a simple \"Health Resource Agent\" referencing sample docs.\n",
    "   - Multi-turn conversation with tracing.\n",
    "   - Cleanup.\n",
    "\n",
    "<img src=\"./seq-diagrams/1-observability.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d687ae",
   "metadata": {},
   "source": [
    "## üîê Authentication Setup\n",
    "\n",
    "Before running the next cell, make sure you're authenticated with Azure CLI. Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "az login --use-device-code\n",
    "```\n",
    "\n",
    "This will provide you with a device code and URL to authenticate in your browser, which is useful for:\n",
    "- Remote development environments\n",
    "- Systems without a default browser\n",
    "- Corporate environments with strict security policies\n",
    "\n",
    "After successful authentication, you can proceed with the notebook cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13f9f3",
   "metadata": {},
   "source": [
    "## 1. Initialization & Setup\n",
    "**Prerequisites**:\n",
    "- A `.env` file containing `AI_FOUNDRY_PROJECT_ENDPOINT` (and optionally `MODEL_DEPLOYMENT_NAME`).\n",
    "- Roles/permissions in Azure AI Foundry that let you do inference & agent creation.\n",
    "- A local environment with `azure-ai-projects`, `azure-ai-inference`, `opentelemetry` packages installed.\n",
    "\n",
    "**What we do**:\n",
    "- Load environment variables.\n",
    "- Initialize `AIProjectClient`.\n",
    "- Check that we can talk to a model (like `gpt-4o`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import UserMessage, CompletionsFinishReason\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "env_path = notebook_path.parent.parent / '.env'  # Adjust path as needed\n",
    "load_dotenv(env_path)\n",
    "\n",
    "project_endpoint = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "if not project_endpoint:\n",
    "    raise ValueError(\"üö® AI_FOUNDRY_PROJECT_ENDPOINT not set in .env.\")\n",
    "\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")\n",
    "\n",
    "# Initialize AIProjectClient with simplified browser-based authentication\n",
    "try:\n",
    "    print(\"üåê Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "    \n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "    \n",
    "    # Create the project client using endpoint\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=project_endpoint,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"‚úÖ Successfully created AIProjectClient!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating AIProjectClient: {e}\")\n",
    "    print(\"üí° Please complete the browser authentication prompt that should appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24461b",
   "metadata": {},
   "source": [
    "## 2. Basic LLM Call\n",
    "We'll do a **quick** chat completion request to confirm everything is working. We'll ask a simple question: \"How many feet are in a mile?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import UserMessage\n",
    "\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "try:\n",
    "    # Use the correct Azure AI Projects SDK pattern\n",
    "    print(\"üîÑ Getting OpenAI client from Azure AI Project...\")\n",
    "    print(f\"ü§ñ Using model: {model_deployment_name}\")\n",
    "    \n",
    "    # Get OpenAI client using the correct method\n",
    "    openai_client = project_client.get_openai_client(api_version=\"2024-10-21\")\n",
    "    \n",
    "    # Create chat completion using OpenAI client pattern\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model_deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful health assistant\"},\n",
    "            {\"role\": \"user\", \"content\": \"How to be healthy in one sentence?\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Successfully created chat completion!\")\n",
    "    print(f\"ü§ñ Assistant: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred: {str(e)}\")\n",
    "    print(\"üí° Troubleshooting tips:\")\n",
    "    print(\"  - Ensure your Azure AI Project has OpenAI connections configured\")\n",
    "    print(\"  - Verify your MODEL_DEPLOYMENT_NAME is correctly deployed\")\n",
    "    print(\"  - Check that you have proper permissions to access the model\")\n",
    "    print(\"  - Make sure you're using the latest azure-ai-projects SDK version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0c8f7",
   "metadata": {},
   "source": [
    "## 3. Observability & Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d366bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install packages exactly as specified in Microsoft documentation if not installed using requirements.txt\n",
    "!pip install azure-ai-projects azure-monitor-opentelemetry opentelemetry-instrumentation-openai-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767143a",
   "metadata": {},
   "source": [
    "## 3.1 Enable OpenTelemetry for Azure AI Inference\n",
    "We set environment variables to ensure:\n",
    "1. **Prompt content** is captured (optional!)\n",
    "2. The **Azure SDK** uses OpenTelemetry as its tracing implementation.\n",
    "3. We call `AIInferenceInstrumentor().instrument()` and `OpenAIInstrumentor().instrument()` to patch and enable the instrumentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef06776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "\n",
    "# CRITICAL: Set environment variables FIRST before any instrumentation\n",
    "print(\"üîß Setting up OpenTelemetry environment variables...\")\n",
    "\n",
    "# Enable content capture (prompts and responses) - MUST be set before instrumentation\n",
    "os.environ[\"OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT\"] = \"true\"\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"  # Keep for backward compatibility\n",
    "\n",
    "# Let the Azure SDK know we want to use OpenTelemetry\n",
    "os.environ[\"AZURE_SDK_TRACING_IMPLEMENTATION\"] = \"opentelemetry\"\n",
    "\n",
    "# Additional OpenTelemetry configuration for better trace visibility\n",
    "os.environ[\"OTEL_PYTHON_LOG_CORRELATION\"] = \"true\"\n",
    "os.environ[\"OTEL_PYTHON_LOG_LEVEL\"] = \"info\"\n",
    "\n",
    "print(\"‚úÖ Environment variables configured for content capture and tracing\")\n",
    "\n",
    "# Microsoft's exact approach: Instrument OpenAI SDK first\n",
    "try:\n",
    "    from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "    OpenAIInstrumentor().instrument()\n",
    "    print(\"‚úÖ OpenAI v2 instrumentation enabled (Microsoft approach).\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è opentelemetry-instrumentation-openai-v2 not available\")\n",
    "    print(\"üí° Installing required package...\")\n",
    "\n",
    "# Then instrument Azure AI Inference\n",
    "AIInferenceInstrumentor().instrument()\n",
    "print(\"‚úÖ Azure AI Inference instrumentation enabled.\")\n",
    "print(\"‚úÖ Content recording enabled for traces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fbc30",
   "metadata": {},
   "source": [
    "### 3.1.2 Point Traces to Console or Local OTLP\n",
    "The simplest is to pipe them to **stdout**. If you want to send them to **Prompty** or **Aspire**, specify the local OTLP endpoint URL (usually `\"http://localhost:4317\"` or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Console tracing setup - following Microsoft's official approach\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "\n",
    "# Set up console tracing for local debugging\n",
    "try:\n",
    "    # Initialize tracer provider if not already done\n",
    "    if not hasattr(trace, '_TRACER_PROVIDER') or trace._TRACER_PROVIDER is None:\n",
    "        tracer_provider = TracerProvider()\n",
    "        trace.set_tracer_provider(tracer_provider)\n",
    "    \n",
    "    # Add console exporter to see traces in stdout using Microsoft's recommended approach\n",
    "    console_exporter = ConsoleSpanExporter()\n",
    "    simple_processor = SimpleSpanProcessor(console_exporter)\n",
    "    trace.get_tracer_provider().add_span_processor(simple_processor)\n",
    "    \n",
    "    print(\"‚úÖ Console telemetry tracing enabled (Microsoft official approach).\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not set up console tracing: {e}\")\n",
    "    print(\"üí° Proceeding without console tracing - telemetry may still work via other means\")\n",
    "\n",
    "try:\n",
    "    # Use the project client to get Azure OpenAI client - this is the recommended approach\n",
    "    openai_client = project_client.get_openai_client(api_version=\"2024-10-21\")\n",
    "    user_prompt = \"What's a simple 5-minute warmup routine?\"\n",
    "    local_resp = openai_client.chat.completions.create(\n",
    "        model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    )\n",
    "    print(\"\\nü§ñ Response:\", local_resp.choices[0].message.content)\n",
    "    print(\"üîç Check console output above for trace information\")\n",
    "except Exception as exc:\n",
    "    print(f\"‚ùå Error in local-tracing example: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0fdd4",
   "metadata": {},
   "source": [
    "## 4. Azure Monitor Tracing (Application Insights)\n",
    "Now we'll set up tracing to **Application Insights**, which will forward your logs to the **Azure AI Foundry** **Tracing** page.\n",
    "\n",
    "**Steps**:\n",
    "1. In AI Foundry, go to your project‚Äôs **Tracing** tab, attach (or create) an **Application Insights** resource.\n",
    "2. In code, call `project_client.telemetry.get_connection_string()` to retrieve the instrumentation key.\n",
    "3. Use `azure.monitor.opentelemetry.configure_azure_monitor(...)` with that connection.\n",
    "4. Make an inference call -> logs appear in the Foundry portal (and in Azure Monitor itself).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install azure-monitor-opentelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903423e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install additional Azure Monitor components for manual setup if needed\n",
    "# %pip install azure-monitor-opentelemetry-exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552014a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from opentelemetry import trace\n",
    "\n",
    "# Get Application Insights connection string and configure Azure Monitor\n",
    "print(\"üîß Setting up Azure Monitor tracing for AI Foundry...\")\n",
    "\n",
    "try:\n",
    "    # Get connection string from project client\n",
    "    connection_string = project_client.telemetry.get_application_insights_connection_string()\n",
    "    \n",
    "    if connection_string:\n",
    "        print(\"‚úÖ Retrieved Application Insights connection string\")\n",
    "        \n",
    "        # Configure Azure Monitor with the connection string\n",
    "        configure_azure_monitor(\n",
    "            connection_string=connection_string,\n",
    "            enable_live_metrics=True\n",
    "        )\n",
    "        print(\"‚úÖ Azure Monitor configured - traces will now be sent to AI Foundry!\")\n",
    "        \n",
    "        # Now send a test trace to verify AI Foundry integration\n",
    "        print(\"\\nüß™ Sending test trace to AI Foundry...\")\n",
    "        \n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        with tracer.start_as_current_span(\"ai_foundry_debug_trace\") as span:\n",
    "            # Use the exact attribute pattern that worked before\n",
    "            span.set_attribute(\"ai.operation.name\", \"notebook_debug\")\n",
    "            span.set_attribute(\"service.name\", \"azure-ai-foundry-notebook\")\n",
    "            span.set_attribute(\"gen_ai.system\", \"openai\")\n",
    "            span.set_attribute(\"gen_ai.operation.name\", \"chat\")\n",
    "            span.set_attribute(\"operation.type\", \"debug_test\")\n",
    "            \n",
    "            # Make an actual OpenAI call to generate a real trace\n",
    "            client = project_client.get_openai_client(api_version=\"2024-10-21\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": \"This is a debug trace to verify Azure AI Foundry integration\"}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Add response details to span\n",
    "            span.set_attribute(\"response.finish_reason\", response.choices[0].finish_reason)\n",
    "            span.set_attribute(\"response.model\", response.model)\n",
    "            \n",
    "            print(f\"‚úÖ Debug trace sent with OpenAI call\")\n",
    "            print(f\"ü§ñ Response: {response.choices[0].message.content[:100]}...\")\n",
    "        \n",
    "        # Force flush to ensure immediate sending\n",
    "        if hasattr(trace.get_tracer_provider(), 'force_flush'):\n",
    "            trace.get_tracer_provider().force_flush(timeout_millis=5000)\n",
    "            print(\"üöÄ Traces flushed to Azure Monitor\")\n",
    "        \n",
    "        print(\"\\nüéØ Check Azure AI Foundry Tracing tab for:\")\n",
    "        print(\"   Operation Name: 'ai_foundry_debug_trace'\")\n",
    "        print(\"   Should appear within 5-10 minutes\")\n",
    "        print(\"\\nüí° This uses the same pattern that worked before!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No Application Insights connection string found\")\n",
    "        print(\"üí° Please ensure your AI Foundry project has Application Insights connected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to configure Azure Monitor: {e}\")\n",
    "    print(\"üí° Please check your AI Foundry project's Application Insights connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dbb932",
   "metadata": {},
   "source": [
    "# 5. Agent-based Example\n",
    "We'll now create a **Health Resource Agent** that references sample docs about recipes or guidelines, then demonstrate:\n",
    "1. Creating an Agent with instructions.\n",
    "2. Creating a conversation thread.\n",
    "3. Running multi-step queries with **observability** enabled.\n",
    "4. Optionally cleaning up resources at the end.\n",
    "\n",
    "> The agent approach is helpful when you want more sophisticated conversation flows or **tool usage** (like file search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ad934",
   "metadata": {},
   "source": [
    "## 5.1 Create Sample Files & Vector Store\n",
    "We'll create dummy `.md` files about recipes/guidelines, then push them into a **vector store** so our agent can do semantic search.\n",
    "\n",
    "(*This portion is a quick summary‚Äîsee [the other file-search tutorial] if you need more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_files():\n",
    "    \"\"\"Create some local .md files with sample text.\"\"\"\n",
    "    recipes_md = (\n",
    "        \"\"\"# Healthy Recipes Database\\n\\n\"\n",
    "        \"## Gluten-Free Recipes\\n\"\n",
    "        \"1. Quinoa Bowl\\n\"\n",
    "        \"   - Ingredients: quinoa, vegetables, olive oil\\n\"\n",
    "        \"   - Instructions: Cook quinoa, add vegetables\\n\\n\"\n",
    "        \"2. Rice Pasta\\n\"\n",
    "        \"   - Ingredients: rice pasta, mixed vegetables\\n\"\n",
    "        \"   - Instructions: Boil pasta, saut√© vegetables\\n\\n\"\n",
    "        \"## Diabetic-Friendly Recipes\\n\"\n",
    "        \"1. Low-Carb Stir Fry\\n\"\n",
    "        \"   - Ingredients: chicken, vegetables, tamari sauce\\n\"\n",
    "        \"   - Instructions: Cook chicken, add vegetables\\n\\n\"\n",
    "        \"## Heart-Healthy Recipes\\n\"\n",
    "        \"1. Baked Salmon\\n\"\n",
    "        \"   - Ingredients: salmon, lemon, herbs\\n\"\n",
    "        \"   - Instructions: Season salmon, bake\\n\\n\"\n",
    "        \"2. Mediterranean Bowl\\n\"\n",
    "        \"   - Ingredients: chickpeas, vegetables, tahini\\n\"\n",
    "        \"   - Instructions: Combine ingredients\\n\"\"\"\n",
    "    )\n",
    "\n",
    "    guidelines_md = (\n",
    "        \"\"\"# Dietary Guidelines\\n\\n\"\n",
    "        \"## General Guidelines\\n\"\n",
    "        \"- Eat a variety of foods\\n\"\n",
    "        \"- Control portion sizes\\n\"\n",
    "        \"- Stay hydrated\\n\\n\"\n",
    "        \"## Special Diets\\n\"\n",
    "        \"1. Gluten-Free Diet\\n\"\n",
    "        \"   - Avoid wheat, barley, rye\\n\"\n",
    "        \"   - Focus on naturally gluten-free foods\\n\\n\"\n",
    "        \"2. Diabetic Diet\\n\"\n",
    "        \"   - Monitor carbohydrate intake\\n\"\n",
    "        \"   - Choose low glycemic foods\\n\\n\"\n",
    "        \"3. Heart-Healthy Diet\\n\"\n",
    "        \"   - Limit saturated fats\\n\"\n",
    "        \"   - Choose lean proteins\\n\"\"\"\n",
    "    )\n",
    "\n",
    "    with open(\"recipes.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(recipes_md)\n",
    "    with open(\"guidelines.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(guidelines_md)\n",
    "\n",
    "    print(\"üìÑ Created sample resource files: recipes.md, guidelines.md\")\n",
    "    return [\"recipes.md\", \"guidelines.md\"]\n",
    "\n",
    "sample_files = create_sample_files()\n",
    "\n",
    "def create_vector_store(files, store_name=\"my_health_resources\"):\n",
    "    try:\n",
    "        uploaded_ids = []\n",
    "        for fp in files:\n",
    "            # Use the correct API pattern from the working notebooks\n",
    "            upl = project_client.agents.files.upload_and_poll(\n",
    "                file_path=fp,\n",
    "                purpose=\"assistants\"  # Use string instead of FilePurpose enum\n",
    "            )\n",
    "            uploaded_ids.append(upl.id)\n",
    "            print(f\"‚úÖ Uploaded: {fp} -> File ID: {upl.id}\")\n",
    "\n",
    "        # Create vector store from these file IDs using correct API pattern\n",
    "        vs = project_client.agents.vector_stores.create_and_poll(\n",
    "            file_ids=uploaded_ids,\n",
    "            name=store_name\n",
    "        )\n",
    "        print(f\"üéâ Created vector store '{store_name}', ID: {vs.id}\")\n",
    "        return vs, uploaded_ids\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating vector store: {e}\")\n",
    "        return None, []\n",
    "\n",
    "vector_store, file_ids = None, []\n",
    "if sample_files:\n",
    "    vector_store, file_ids = create_vector_store(sample_files, store_name=\"health_resources_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145eb186",
   "metadata": {},
   "source": [
    "## 5.2 Create a Health Resource Agent\n",
    "We'll create a **FileSearchTool** referencing the vector store, then create an agent with instructions that it should:\n",
    "1. Provide disclaimers.\n",
    "2. Offer general nutrition or recipe tips.\n",
    "3. Cite sources if possible.\n",
    "4. Encourage professional consultation for deeper medical advice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f604175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_health_agent(vs_id):\n",
    "    try:\n",
    "        # Create agent using the working pattern from file-search notebook\n",
    "        # Use tools as simple dictionary instead of FileSearchTool class\n",
    "        agent = project_client.agents.create_agent(\n",
    "            model=os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "            name=\"health-search-agent\",\n",
    "            instructions=\"\"\"\n",
    "                You are a health resource advisor with access to dietary and recipe files.\n",
    "                You:\n",
    "                1. Always present disclaimers (you're not a medical professional)\n",
    "                2. Provide references to files when possible\n",
    "                3. Focus on general nutrition or recipe tips.\n",
    "                4. Encourage professional consultation for more detailed advice.\n",
    "            \"\"\",\n",
    "            tools=[{\"type\": \"file_search\"}]  # Use dictionary instead of FileSearchTool class\n",
    "        )\n",
    "        print(f\"üéâ Created agent '{agent.name}' with ID: {agent.id}\")\n",
    "        print(\"üìã Vector store will be attached at message level for better compatibility\")\n",
    "        return agent\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating health agent: {e}\")\n",
    "        return None\n",
    "\n",
    "health_agent = None\n",
    "if vector_store:\n",
    "    health_agent = create_health_agent(vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6995a6",
   "metadata": {},
   "source": [
    "## 5.3 Using the Agent with Enhanced Tracing\n",
    "Let's create a new conversation **thread** and ask the agent some questions. We've enhanced this section to add **explicit tracing spans** around agent operations to ensure they appear in **Azure AI Foundry**.\n",
    "\n",
    "**Enhanced Tracing Features**:\n",
    "- Custom spans for agent demo, queries, and run execution\n",
    "- Detailed attributes including agent ID, thread ID, query text, and file attachment counts\n",
    "- Error tracking and status monitoring\n",
    "- Hierarchical span structure for better trace visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e5b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thread():\n",
    "    try:\n",
    "        # Use the correct API pattern for thread creation\n",
    "        thread = project_client.agents.threads.create()\n",
    "        print(f\"üìù Created new thread, ID: {thread.id}\")\n",
    "        return thread\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not create thread: {e}\")\n",
    "        return None\n",
    "\n",
    "def ask_question_with_files(thread_id, agent_id, user_question, file_ids_list):\n",
    "    try:\n",
    "        # Create explicit tracing span for agent operations\n",
    "        from opentelemetry import trace\n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        with tracer.start_as_current_span(\"agent_file_search_query\") as span:\n",
    "            # Add custom attributes for better tracing visibility\n",
    "            span.set_attribute(\"operation.type\", \"agent_file_search\")\n",
    "            span.set_attribute(\"agent.id\", agent_id)\n",
    "            span.set_attribute(\"thread.id\", thread_id)\n",
    "            span.set_attribute(\"query.text\", user_question)\n",
    "            span.set_attribute(\"file_attachments.count\", len(file_ids_list))\n",
    "            \n",
    "            # Create message with file attachments for search - using working pattern\n",
    "            attachments = []\n",
    "            for file_id in file_ids_list:\n",
    "                attachments.append({\n",
    "                    \"file_id\": file_id,\n",
    "                    \"tools\": [{\"type\": \"file_search\"}]\n",
    "                })\n",
    "            \n",
    "            # Add user message with file attachments\n",
    "            msg = project_client.agents.messages.create(\n",
    "                thread_id=thread_id,\n",
    "                role=\"user\",\n",
    "                content=user_question,\n",
    "                attachments=attachments\n",
    "            )\n",
    "            print(f\"User asked: '{user_question}' with {len(file_ids_list)} file attachments\")\n",
    "            \n",
    "            # Create & process a run with tracing\n",
    "            with tracer.start_as_current_span(\"agent_run_execution\") as run_span:\n",
    "                run_span.set_attribute(\"agent.model\", os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o\"))\n",
    "                \n",
    "                run = project_client.agents.runs.create_and_process(\n",
    "                    thread_id=thread_id,\n",
    "                    agent_id=agent_id\n",
    "                )\n",
    "                \n",
    "                # Add run details to span\n",
    "                run_span.set_attribute(\"run.id\", run.id)\n",
    "                run_span.set_attribute(\"run.status\", run.status)\n",
    "                \n",
    "                print(f\"Run finished with status: {run.status}\")\n",
    "                if run.last_error:\n",
    "                    print(\"Error details:\", run.last_error)\n",
    "                    run_span.set_attribute(\"run.error\", str(run.last_error))\n",
    "                \n",
    "                return run\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error asking question: {e}\")\n",
    "        if 'span' in locals():\n",
    "            span.set_attribute(\"error\", True)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "        return None\n",
    "\n",
    "if health_agent:\n",
    "    # Create thread with tracing context\n",
    "    from opentelemetry import trace\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    with tracer.start_as_current_span(\"health_agent_demo\") as demo_span:\n",
    "        demo_span.set_attribute(\"operation.type\", \"agent_demo\")\n",
    "        demo_span.set_attribute(\"agent.name\", \"health-search-agent\")\n",
    "        \n",
    "        thread = create_thread()\n",
    "        if thread:\n",
    "            # Let's ask a few sample questions with file search - all within tracing context\n",
    "            queries = [\n",
    "                \"Could you suggest a gluten-free lunch recipe?\",\n",
    "                \"Show me some heart-healthy meal ideas.\",\n",
    "                \"What guidelines do you have for someone with diabetes?\"\n",
    "            ]\n",
    "            \n",
    "            demo_span.set_attribute(\"queries.count\", len(queries))\n",
    "            \n",
    "            for idx, q in enumerate(queries):\n",
    "                if file_ids:\n",
    "                    with tracer.start_as_current_span(f\"query_{idx+1}\") as query_span:\n",
    "                        query_span.set_attribute(\"query.index\", idx + 1)\n",
    "                        query_span.set_attribute(\"query.text\", q)\n",
    "                        ask_question_with_files(thread.id, health_agent.id, q, file_ids)\n",
    "\n",
    "    print(\"\\nüéØ Agent traces should now be visible in Azure AI Foundry!\")\n",
    "    print(\"   Look for traces with operation names:\")\n",
    "    print(\"   - 'health_agent_demo' (overall demo)\")\n",
    "    print(\"   - 'agent_file_search_query' (each question)\")\n",
    "    print(\"   - 'agent_run_execution' (agent processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c61d8d",
   "metadata": {},
   "source": [
    "### 5.3.1 Viewing the Conversation & Tracing Status\n",
    "We can retrieve the conversation messages to see how the agent responded, check if it cited file passages, and **verify that our tracing is working properly**.\n",
    "\n",
    "**üîç Tracing Troubleshooting**:\n",
    "\n",
    "If traces don't appear in Azure AI Foundry, check:\n",
    "\n",
    "1. **Application Insights Connection**: Ensure your AI Foundry project has an Application Insights resource connected\n",
    "2. **Connection String**: The `project_client.telemetry.get_connection_string()` should return a valid connection string\n",
    "3. **Time Delay**: Traces can take 2-5 minutes to appear in the portal\n",
    "4. **Trace Names**: Look for these specific operation names:\n",
    "   - `health_agent_demo` (overall agent demo)\n",
    "   - `agent_file_search_query` (each question asked)\n",
    "   - `agent_run_execution` (agent processing)\n",
    "   - `display_conversation` (conversation display)\n",
    "\n",
    "5. **Manual Verification**: Check Application Insights directly in Azure Portal if AI Foundry doesn't show traces\n",
    "6. **Environment Variables**: Ensure `OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true` is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c57935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_thread(thread_id):\n",
    "    try:\n",
    "        # Add tracing context for message retrieval\n",
    "        from opentelemetry import trace\n",
    "        tracer = trace.get_tracer(__name__)\n",
    "        \n",
    "        with tracer.start_as_current_span(\"display_conversation\") as span:\n",
    "            span.set_attribute(\"operation.type\", \"display_thread\")\n",
    "            span.set_attribute(\"thread.id\", thread_id)\n",
    "            \n",
    "            # Use the correct API pattern for listing messages\n",
    "            messages = project_client.agents.messages.list(thread_id=thread_id)\n",
    "            print(\"\\nüó£Ô∏è Conversation:\")\n",
    "            \n",
    "            # Convert to list and reverse to show chronological order\n",
    "            message_list = list(messages)\n",
    "            span.set_attribute(\"messages.count\", len(message_list))\n",
    "            \n",
    "            for m in reversed(message_list):\n",
    "                if m.content:\n",
    "                    last_content = m.content[-1]\n",
    "                    if hasattr(last_content, \"text\"):\n",
    "                        print(f\"[{m.role.upper()}]: {last_content.text.value}\\n\")\n",
    "\n",
    "            print(\"\\nüìé Checking for citations...\")\n",
    "            citation_count = 0\n",
    "            for m in message_list:\n",
    "                if m.content:\n",
    "                    for content_item in m.content:\n",
    "                        if hasattr(content_item, \"text\") and hasattr(content_item.text, \"annotations\"):\n",
    "                            for annotation in content_item.text.annotations:\n",
    "                                citation_count += 1\n",
    "                                if hasattr(annotation, \"file_citation\"):\n",
    "                                    print(f\"- Citation {citation_count}: '{annotation.text}' from file ID: {annotation.file_citation.file_id}\")\n",
    "                                else:\n",
    "                                    print(f\"- Citation {citation_count}: '{annotation.text}'\")\n",
    "            \n",
    "            span.set_attribute(\"citations.count\", citation_count)\n",
    "            \n",
    "            if citation_count == 0:\n",
    "                print(\"No explicit citations found - checking for file content usage...\")\n",
    "                content_match_count = 0\n",
    "                for m in message_list:\n",
    "                    if m.role == \"assistant\" and m.content:\n",
    "                        for content_item in m.content:\n",
    "                            if hasattr(content_item, \"text\"):\n",
    "                                text = content_item.text.value.lower()\n",
    "                                if any(keyword in text for keyword in ['quinoa', 'salmon', 'gluten-free', 'diabetic', 'heart-healthy']):\n",
    "                                    print(f\"‚úÖ Agent appears to be using file content (found relevant keywords)\")\n",
    "                                    content_match_count += 1\n",
    "                                    break\n",
    "                \n",
    "                span.set_attribute(\"content_matches.count\", content_match_count)\n",
    "                                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not display thread: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Add error to span if it exists\n",
    "        if 'span' in locals():\n",
    "            span.set_attribute(\"error\", True)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "\n",
    "# If we created a thread above, let's read it with tracing\n",
    "if 'health_agent' in globals() and health_agent and 'thread' in globals() and thread:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä DISPLAYING CONVERSATION WITH TRACING\")\n",
    "    print(\"=\"*50)\n",
    "    display_thread(thread.id)\n",
    "    \n",
    "    print(\"\\nüîç **TRACING STATUS CHECK**:\")\n",
    "    print(f\"‚úÖ Azure Monitor connection: {'‚úì' if 'connection_string' in globals() and connection_string else '‚úó'}\")\n",
    "    print(f\"‚úÖ OpenTelemetry tracer: {'‚úì' if 'tracer' in globals() else '‚úó'}\")\n",
    "    print(f\"‚úÖ Agent operations traced: ‚úì (with custom spans)\")\n",
    "    print(f\"‚úÖ Thread ID for reference: {thread.id}\")\n",
    "    \n",
    "    print(\"\\nüéØ **WHERE TO FIND TRACES**:\")\n",
    "    print(\"1. Go to https://ai.azure.com\")  \n",
    "    print(\"2. Navigate to your project\")\n",
    "    print(\"3. Click 'Tracing' in the left sidebar\")\n",
    "    print(\"4. Look for these trace operation names:\")\n",
    "    print(\"   - 'health_agent_demo' (overall demo)\")\n",
    "    print(\"   - 'agent_file_search_query' (individual questions)\")\n",
    "    print(\"   - 'agent_run_execution' (agent processing)\")\n",
    "    print(\"   - 'display_conversation' (this display function)\")\n",
    "    print(\"5. Filter by time range if needed (last 15-30 minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7420c39",
   "metadata": {},
   "source": [
    "# 6. Cleanup\n",
    "If desired, we can remove the vector store, files, and agent to keep things tidy. (In a real solution, you might keep them around.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources():\n",
    "    try:\n",
    "        # Use the correct API pattern for cleanup\n",
    "        if 'vector_store' in globals() and vector_store:\n",
    "            project_client.agents.vector_stores.delete(vector_store.id)\n",
    "            print(\"üóëÔ∏è Deleted vector store.\")\n",
    "\n",
    "        if 'file_ids' in globals() and file_ids:\n",
    "            for fid in file_ids:\n",
    "                project_client.agents.files.delete(fid)\n",
    "            print(\"üóëÔ∏è Deleted uploaded files.\")\n",
    "\n",
    "        if 'health_agent' in globals() and health_agent:\n",
    "            project_client.agents.delete_agent(health_agent.id)\n",
    "            print(\"üóëÔ∏è Deleted health agent.\")\n",
    "\n",
    "        if 'sample_files' in globals() and sample_files:\n",
    "            for sf in sample_files:\n",
    "                if os.path.exists(sf):\n",
    "                    os.remove(sf)\n",
    "            print(\"üóëÔ∏è Deleted local sample files.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning up: {e}\")\n",
    "\n",
    "\n",
    "cleanup_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956d0ec",
   "metadata": {},
   "source": [
    "# üéâ Wrap-Up\n",
    "We've demonstrated:\n",
    "1. **Basic LLM calls** with `AIProjectClient`.\n",
    "2. **Listing connections** in your Azure AI Foundry project.\n",
    "3. **Observability & tracing** in both local (console, OTLP endpoint) and cloud (App Insights) contexts.\n",
    "4. A quick **Agent** scenario that uses a vector store for searching sample docs.\n",
    "\n",
    "## Next Steps\n",
    "- Check the **Tracing** tab in your Azure AI Foundry portal to see the logs.\n",
    "- Explore advanced queries in Application Insights.\n",
    "- Use [Prompty](https://github.com/microsoft/prompty) or [Aspire](https://learn.microsoft.com/dotnet/aspire/) for local telemetry dashboards.\n",
    "- Incorporate this approach into your **production** GenAI pipelines!\n",
    "\n",
    "> üèãÔ∏è **Health Reminder**: The LLM's suggestions are for demonstration only. For real health decisions, consult a professional.\n",
    "\n",
    "Happy Observing & Tracing! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "name": "Observability_and_Tracing_Comprehensive"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
